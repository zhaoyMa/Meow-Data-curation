#!/usr/bin/env python3
# -*- coding: utfâ€‘8 -*-
"""
ç»Ÿè®¡ merged_train.json å’Œ merged_test.json ä¸­æ¯æ¡è®°å½•çš„ token æ•°é‡ï¼Œå¹¶è¾“å‡ºç»“æœã€‚
"""

import json
from transformers import AutoTokenizer
from tqdm import tqdm

# æŒ‡å®š tokenizer è·¯å¾„
model_path = "/nfs/mazhaoyu/llms/Qwen/Qwen3-8B"

# åŠ è½½ tokenizerï¼Œä»…éœ€å ç”¨éå¸¸å°‘çš„èµ„æº
tokenizer = AutoTokenizer.from_pretrained(model_path)

# éœ€è¦å¤„ç†çš„ä¸¤ä¸ªæ–‡ä»¶
files = {
    "train": "/nfs/mazhaoyu/LLaMA-Factory/data/merged_train.json",
    "test":  "/nfs/mazhaoyu/LLaMA-Factory/data/merged_test.json",
}

# ç»“æœå­—å…¸ï¼Œç”¨äºå­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
stats = {}

for split, path in files.items():
    token_counts = []
    total_tokens = 0
    num_samples = 0
    mz32k = 0
    # åŠ è½½ JSON æ–‡ä»¶ï¼ˆå‡è®¾æ¯è¡Œä¸€æ¡ JSON æˆ–æ•´ä¸ªæ–‡ä»¶ä¸º listï¼‰
    with open(path, "r", encoding="utfâ€‘8") as f:
        data = json.load(f)
        if isinstance(data, dict):
            # å¦‚æœæ˜¯ dict åŒ…å« key: list
            data = data.get(split, data.get("items", []))
        elif not isinstance(data, list):
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„ JSON æ ¼å¼ï¼š{path}")
    
    # éå†æ‰€æœ‰æ–‡æœ¬å­—æ®µ
    for rec in tqdm(data, desc=f"Tokenizing {split}", unit="rec"):
        text = rec.get("text") or rec.get("input") or rec.get("instruction") or rec.get("content")
        if text is None:
            continue
        tokens = tokenizer.encode(text, add_special_tokens=False)
        cnt = len(tokens)
        if cnt>32768:
            mz32k+=1
        token_counts.append(cnt)
        total_tokens += cnt
        num_samples += 1
    
    avg = total_tokens / max(num_samples, 1)
    stats[split] = {
        "samples": num_samples,
        "total_tokens": total_tokens,
        "avg_tokens": avg,
        "token_counts": token_counts,
        "mz32k": mz32k,
    }

# è¾“å‡ºç»“æœ
print("ğŸ“Š Tokenization ç»Ÿè®¡ç»“æœï¼š")
for split, st in stats.items():
    print(f"  â€¢ {split}: å¤§äº32k:{st['mz32k']}æ ·æœ¬æ•°={st['samples']},  å¹³å‡æ¯æ¡={st['avg_tokens']:.2f}")
